{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Failure Prediction with ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents</h3>\n    \n* [Data](#0)\n* [What Problem We Have and Which Metric to Use?](#1)\n\n* [Exploratory Data Analysis](#2)\n    * [Target Variable](#3)\n    * [Numerical Features](#4)\n    * [Categorical Features](#5)    \n    \n* [Model Selection](#6)    \n    * [Baseline Model](#7)\n    * [Logistic & Linear Discriminant & SVC & KNN](#8)\n    * [Logistic & Linear Discriminant & SVC & KNN with Scaler](#9)    \n    * [Ensemble Models (AdaBoost & Gradient Boosting & Random Forest & Extra Trees)](#10)\n    * [Famous Trio (XGBoost & LightGBM & Catboost)](#11)\n    * [CATBOOST](#12)\n    * [Catboost HyperParameter Tuning with OPTUNA](#13)\n    * [Feature Importance](#14)    \n    * [Model Comparison](#15)  \n    \n    \n\n\n* [Conclusion](#16)\n\n* [References & Further Reading](#17)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Data</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"# Heart Failure Prediction Dataset","metadata":{}},{"cell_type":"markdown","source":"DATA DICTONARY\t\t\t\t\t\t\n\t\t\t\t\t\t\n1\t**Age**: \t\t\tAge of the patient [years] \t\t\n2\t**Sex**:  \t\t\t Sex of the patient [M: Male, F: Female] \t\t\n3\t**ChestPainType**: \t\t\t[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic] \t\t\n4\t**RestingBP**:\t\t\tResting blood pressure [mm Hg] \t\t\n5\t**Cholesterol**:\t\t\tSerum cholesterol [mm/dl] \t\t\n6\t**FastingBS**:\t\t\t Fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\t\t\n7\t**RestingECG**:\t\t\t Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria] \t\t\n8\t**MaxHR**:\t\t\tMaximum heart rate achieved [Numeric value between 60 and 202]\t\t\n9\t**ExerciseAngina**:\t\t\tExercise-induced angina [Y: Yes, N: No]\t\t\n10\t**Oldpeak**:\t\t\t ST [Numeric value measured in depression] (\t\t\n11\t**ST_Slope**:\t\t\t The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping] \t\t\n12\t**HeartDisease**:\t\t\t Output class [1: heart disease, 0: Normal] \t\t\n\n\n**Context**\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\nReference: https://www.kaggle.com/fedesoriano/heart-failure-prediction","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>What Problem We Have and Which Metric to Use?</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- Based on the data and data dictionary, We have a classification problem.\n- We wil make classification on the target variable **Heart Disease**\n- And we will build a model to get best calssification possible on the target variable.\n- For that we will look at the balance of the target variable.\n- As we will see later, our target variable has balanced or balanced like data\n- For that reason we will use **Accuracy score**.\n- [For the detailed info about the evaluation metrics](https://www.kaggle.com/kaanboke/the-most-common-evaluation-metrics-a-gentle-intro)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"#!pip install pyforest","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:37:08.910476Z","iopub.execute_input":"2021-10-16T11:37:08.910822Z","iopub.status.idle":"2021-10-16T11:37:08.915244Z","shell.execute_reply.started":"2021-10-16T11:37:08.91079Z","shell.execute_reply":"2021-10-16T11:37:08.914295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install pyforest\n# 1-Import Libraies\n\n#!pip install lightgbm\n#!pip install catboost\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\n%matplotlib notebook\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n#Model Selection\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.model_selection import KFold, cross_val_predict\n\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n\n#Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.neural_network import MLPRegressor\n\n#Scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n\n#Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve, precision_recall_curve\nfrom sklearn.metrics import accuracy_score, recall_score, average_precision_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score \n\n\n#Importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n#Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n#!pip install termcolor\nfrom termcolor import colored\nimport missingno as msno \n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import plotly.express as px\n# import scipy.stats as stats\n# import pyforest\n# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, RobustScaler\n# from sklearn.model_selection import KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\n# from sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\n# from sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\n# from sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\n# from sklearn.pipeline import make_pipeline\n# from sklearn.compose import make_column_transformer\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.svm import SVR\n# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostClassifier\n# from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n# from xgboost import XGBRegressor, XGBClassifier\n# from xgboost import plot_importance\n# from sklearn.pipeline import Pipeline\n# from sklearn.tree import plot_tree\n# from sklearn.impute import SimpleImputer, KNNImputer\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.svm import SVC\n# #importing plotly and cufflinks in offline mode\n# import cufflinks as cf\n# import plotly.offline\n# cf.go_offline()\n# cf.set_config_file(offline=False, world_readable=True)\n# import warnings\n# warnings.filterwarnings('ignore')\n# warnings.warn(\"this will not show\")\n# plt.rcParams[\"figure.figsize\"] = (10,6)\n# pd.set_option('max_colwidth',200)\n# # pd.set_option('display.max_rows', 100) # if you wish to see more rows rather than default, just uncomment this line.\n# pd.set_option('display.max_columns', 200)\n# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n# import colorama\n# from colorama import Fore, Style  # maakes strings colored\n# # !pip3 install termcolor\n# from termcolor import colored\n\n\n\n\n\n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n\n\n# from sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\n# from sklearn.impute import SimpleImputer\n# from sklearn.pipeline import Pipeline\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n# from sklearn.impute import SimpleImputer\n# from sklearn.dummy import DummyClassifier\n# from imblearn.over_sampling import SMOTE\n\n# from sklearn.ensemble import AdaBoostClassifier\n# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import ExtraTreesClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n\n# import optuna\n# from xgboost import XGBClassifier\n# from lightgbm import LGBMClassifier\n# from catboost import CatBoostClassifier\n\n# from sklearn.pipeline import make_pipeline\n# from sklearn.pipeline import Pipeline\n# from sklearn.compose import make_column_transformer\n\n# from sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\n# from sklearn.metrics import accuracy_score,classification_report\n\n# #importing plotly and cufflinks in offline mode\n# import cufflinks as cf\n# import plotly.offline\n# cf.go_offline()\n# cf.set_config_file(offline=False, world_readable=True)\n\n\n# import plotly \n# import plotly.express as px\n# import plotly.graph_objs as go\n# import plotly.offline as py\n# from plotly.offline import iplot\n# from plotly.subplots import make_subplots\n# import plotly.figure_factory as ff\n\n# import missingno as msno\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T11:37:44.630065Z","iopub.execute_input":"2021-10-16T11:37:44.63046Z","iopub.status.idle":"2021-10-16T11:37:44.690657Z","shell.execute_reply.started":"2021-10-16T11:37:44.630427Z","shell.execute_reply":"2021-10-16T11:37:44.689622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful Functions","metadata":{}},{"cell_type":"code","source":"## Useful Functions\n\n###############################################################################\n\ndef first_looking(column):\n    print(\"column name    : \", column) \n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[column].isnull().sum()/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[column].isnull().sum())\n    print(\"num_of_uniques : \", df[column].nunique())\n    print(\"value_counts : \", df[column].value_counts(dropna = False).head())\n    \n# for i in df.columns:\n#     first_looking(i)\n\n###############################################################################\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\n###############################################################################\n\ndef perc_nans(serial):  # Ex:perc_nans(df['kW'])\n    # display percentage of nans in a Series\n    return serial.isnull().sum()/serial.shape[0]*100\n\ndef perc_nans_byLimitless(df):\n    return df.isnull().sum()/df.shape[0]*100\n\ndef perc_nans_byLimit(df, limit):\n    missing = df.isnull().sum()*100/df.shape[0]\n    return missing.loc[lambda x : x >= limit]\n\n# perc_nans_byLimit(df, 90)\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"duplicates were dropped\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"No duplicates\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary drop some columns!!!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)\n        else:\n            print(df.isnull().sum()[i], '%, percentage of missing values of', i ,'less than limit', limit, '%, so we will keep it.')\n    print('New shape after missing value control:', df.shape)\n\n###############################################################################\n\ndef fill_median(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (median) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        median = list(df[cond][col_name].median())\n        if median != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].median()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].median()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))\n    \n###############################################################################\n\ndef fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))\n    \n###############################################################################\n\ndef fill_prop(df, group_col, col_name):\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        df.loc[cond, col_name] = df.loc[cond, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    df[col_name] = df[col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))\n    \n###############################################################################\n\ndef fill(df, group_col1, group_col2, col_name, method): # method can be \"mode\" or \"median\" or \"ffill\"\n    if method == \"mode\":\n        for group1 in list(df[group_col1].unique()):\n            for group2 in list(df[group_col2].unique()):\n                cond1 = df[group_col1]==group1\n                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n                mode1 = list(df[cond1][col_name].mode())\n                mode2 = list(df[cond2][col_name].mode())\n                if mode2 != []:\n                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond2][col_name].mode()[0])\n                elif mode1 != []:\n                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond1][col_name].mode()[0])\n                else:\n                    df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[col_name].mode()[0])\n                \n    elif method == \"median\":\n        for group1 in list(df[group_col1].unique()):\n            for group2 in list(df[group_col2].unique()):\n                cond1 = df[group_col1]==group1\n                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n                df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(df[cond2][col_name].median()).fillna(df[cond1][col_name].median()).fillna(df[col_name].median())\n                \n    elif method == \"ffill\":           \n        for group1 in list(df[group_col1].unique()):\n            for group2 in list(df[group_col2].unique()):\n                cond2 = (df[group_col1]==group1) & (df[group_col2]==group2)\n                df.loc[cond2, col_name] = df.loc[cond2, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n                \n        for group1 in list(df[group_col1].unique()):\n            cond1 = df[group_col1]==group1\n            df.loc[cond1, col_name] = df.loc[cond1, col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")            \n           \n        df[col_name] = df[col_name].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    \n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))\n    \n###############################################################################\n\ndef model_validation(y_train, y_train_pred, y_test, y_test_pred, model_name):\n    \n    scores =  {f\"{model_name}_train\": {\"R2\" : r2_score(y_train, y_train_pred),\n    \"rmse\" : np.sqrt(mean_squared_error(y_train, y_train_pred)),\n    \"mse\" : mean_squared_error(y_train, y_train_pred), \n    \"mae\" : mean_absolute_error(y_train, y_train_pred)},\n    \n    f\"{model_name}_test\": {\"R2\" : r2_score(y_test, y_test_pred),\n    \"rmse\" : np.sqrt(mean_squared_error(y_test, y_test_pred)),\n    \"mse\" : mean_squared_error(y_test, y_test_pred),\n    \"mae\" : mean_absolute_error(y_test, y_test_pred)}}\n     \n    return pd.DataFrame(scores)\n\n# lm = model_validation(y_train, y_train_pred, y_test, y_test_pred, 'lm')\n\n# pd.concat([lm, rs, rcvs, lss, lcvs, es, ecvs], axis = 1)\n\n###############################################################################\n\ndef get_classification_report(y_test, y_test_pred):\n    from sklearn import metrics\n    report = metrics.classification_report(y_test, y_test_pred, output_dict=True)\n    df_classification_report = pd.DataFrame(report).transpose()\n    #df_classification_report = df_classification_report.sort_values(by=['f1-score'], ascending=False)\n    return df_classification_report\n\n###############################################################################\n\ndef shape_control():\n    print('df.shape:', df.shape)\n    print('X.shape:', X.shape)\n    print('y.shape:', y.shape)\n    print('X_train.shape:', X_train.shape)\n    print('y_train.shape:', y_train.shape)\n    print('X_test.shape:', X_test.shape)\n    print('y_test.shape:', y_test.shape)\n    try:\n        print('y_test_pred.shape:', y_test_pred.shape)\n    except:\n        print()\n        \n###############################################################################\n\ndef calc_predict():\n    return accuracy_score(y_test, y_test_pred), recall_score(y_test, y_test_pred, average='weighted'), f1_score(y_test, y_test_pred, average='weighted')\n    \ndef get_report():\n    from sklearn import metrics\n    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n    y_train_pred = model.predict(X_train_scaled)\n    try:\n        y_train_pred_proba = model.predict_proba(X_train_scaled)\n    except:\n        print()\n    try:\n        precision, recall, _ = precision_recall_curve(y_train, y_train_pred_proba[:,1])\n    except:\n        print() \n    try:\n        y_test_pred_proba = model.predict_proba(X_test_scaled)\n    except:\n        print()\n    try:\n        precision, recall, _ = precision_recall_curve(y_test, y_test_pred_proba[:,1])\n    except:\n        print()  \n    print('Model:', model.get_params, '\\n')\n    try:\n        print('model.best_params_:', model.best_params_, '\\n')\n    except:\n        print()\n    print(\"Train:\")\n    print('rmse:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n    print('accuracy:', accuracy_score(y_train, y_train_pred))\n    try:\n        print('roc_auc_score:',roc_auc_score(y_train, y_train_pred_proba[:,1]))\n    except:\n        print()\n    try:\n        print('roc_auc_recall_precision_score:',auc(recall, precision),'\\n')\n    except:\n        print()\n    print('confusion_matrix:\\n\\n', confusion_matrix(y_train, y_train_pred), '\\n')\n    print('classification_report:\\n\\n', classification_report(y_train, y_train_pred),'\\n')\n    print()\n    print(\"Test:\")\n    print('rmse:', np.sqrt(mean_squared_error(y_test, y_test_pred))) \n    print('accuracy:', accuracy_score(y_test, y_test_pred))\n    try:\n        print('roc_auc_score:',roc_auc_score(y_test, y_test_pred_proba[:,1]))\n    except:\n        print() \n    try:\n        print('roc_auc_recall_precision_score:',auc(recall, precision),'\\n')\n    except:\n        print() \n    print('confusion_matrix:\\n\\n', confusion_matrix(y_test, y_test_pred), '\\n')\n    print('classification_report:\\n\\n', classification_report(y_test, y_test_pred))\n\ndef train_control_table():\n    y_train_pred = model.predict(X_train_scaled)\n    y_train_pred = pd.DataFrame(y_train_pred)\n    y_train_pred.rename(columns = {0: 'y_train_pred'}, inplace = True)\n    return pd.concat([X_train, y_train, y_train_pred.set_index(y_train.index)], axis=1)\n\ndef test_control_table():\n    y_test_pred = model.predict(X_test_scaled)\n    y_test_pred = pd.DataFrame(y_test_pred)\n    y_test_pred.rename(columns = {0: 'y_test_pred'}, inplace = True)\n    return pd.concat([X_test, y_test, y_test_pred.set_index(y_test.index)], axis=1)\n\ndef get_roc_curve():\n    from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n    plot_roc_curve(model, X_train_scaled, y_train);\n    plot_precision_recall_curve(model, X_train_scaled, y_train);\n\n    plot_roc_curve(model, X_test_scaled, y_test);\n    plot_precision_recall_curve(model, X_test_scaled, y_test);\n    \ndef visualizer(model):\n    from yellowbrick.classifier import ClassPredictionError\n    visualizer = ClassPredictionError(model)\n    # Fit the training data to the visualizer\n    visualizer.fit(X_train_scaled, y_train)\n    # Evaluate the model on the test data\n    visualizer.score(X_test_scaled, y_test)\n    # Draw visualization\n    visualizer.poof();\n    \n###############################################################################\n\ndef feature_importances():\n    df_fi = pd.DataFrame(index=X.columns, \n                         data=model.feature_importances_, \n                         columns=[\"Feature Importance\"]).sort_values(\"Feature Importance\")\n\n    return df_fi.sort_values(by=\"Feature Importance\", ascending=False).T\n\ndef feature_importances_bar():\n    df_fi = pd.DataFrame(index=X.columns, \n                         data=model.feature_importances_, \n                         columns=[\"Feature Importance\"]).sort_values(\"Feature Importance\")\n    sns.barplot(data = df_fi, \n                x = df_fi.index, \n                y = 'Feature Importance', \n                order=df_fi.sort_values('Feature Importance', ascending=False).reset_index()['index'])\n    plt.xticks(rotation = 90)\n    plt.tight_layout()\n    plt.show();\n    \n    \n    \n###############################################################################\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n    z_scores = stats.zscore(df[col].dropna())\n    threshold_list = []\n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))/df_outlier.outlier_count*100\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n               colors=\"r\", ls = \":\"\n              )\n    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n                                                                   (np.round(percentile_threshold, 3), \n                                                                    np.round(100-percentile_threshold, 3))), \n                 (best_treshold, df_outlier.outlier_count.max()/2))\n    #plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, outlier_limit, percentile_threshold)\n\ndef outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n    plt.show()\n\n###############################################################################\n\ndef plot_multiclass_roc(model, X_test_scaled, y_test, n_classes, figsize=(5,5)):\n    y_score = model.decision_function(X_test_scaled)\n\n    # structures\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    # calculate dummies once\n    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # roc for each class\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic example')\n    for i in range(n_classes):\n        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n    ax.legend(loc=\"best\")\n    ax.grid(alpha=.4)\n    sns.despine()\n    plt.show()\n    \n    \ndef plot_multiclass_roc_for_tree(model, X_test_scaled, y_test, n_classes, figsize=(5,5)):\n    y_score = model.predict_proba(X_test_scaled)\n\n    # structures\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    # calculate dummies once\n    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # roc for each class\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic example')\n    for i in range(n_classes):\n        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n    ax.legend(loc=\"best\")\n    ax.grid(alpha=.4)\n    sns.despine()\n    plt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:38:04.788536Z","iopub.execute_input":"2021-10-16T11:38:04.788828Z","iopub.status.idle":"2021-10-16T11:38:05.056175Z","shell.execute_reply.started":"2021-10-16T11:38:04.788795Z","shell.execute_reply":"2021-10-16T11:38:05.054983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:38:17.976547Z","iopub.execute_input":"2021-10-16T11:38:17.976857Z","iopub.status.idle":"2021-10-16T11:38:18.009752Z","shell.execute_reply.started":"2021-10-16T11:38:17.976809Z","shell.execute_reply":"2021-10-16T11:38:18.008705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kurtosis are of three types:\nMesokurtic: When the tails of the distibution is similar to the normal distribution then it is mesokurtic. The kutosis for normal distibution is 3.\nLeptokurtic: If the kurtosis is greater than 3 then it is leptokurtic. In this case, the tails will be heaviour than the normal distribution which means lots of outliers are present in the data. It can be recognized as thin bell shaped distribution with peak higher than normal distribution.\nPlatykurtic: Kurtosis will be less than 3 which implies thinner tail or lack of outliers than normal distribution.In case of platykurtic, bell shaped distribution will be broader and peak will be lower than the mesokurtic.\nHair et al. (2010) and Bryne (2010) argued that data is considered to be normal if Skewness is between ‐2 to +2 and Kurtosis is between ‐7 to +7.\nMulti-normality data tests are performed using leveling asymmetry tests (skewness < 3), (Kurtosis between -2 and 2) and Mardia criterion (< 3). Source Chemingui, H., & Ben lallouna, H. (2013).\nSkewness and kurtosis index were used to identify the normality of the data. The result suggested the deviation of data from normality was not severe as the value of skewness and kurtosis index were below 3 and 10 respectively (Kline, 2011). Source Yadav, R., & Pathak, G. S. (2016).","metadata":{}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:41:26.10039Z","iopub.execute_input":"2021-10-16T11:41:26.10075Z","iopub.status.idle":"2021-10-16T11:41:26.107189Z","shell.execute_reply.started":"2021-10-16T11:41:26.100718Z","shell.execute_reply":"2021-10-16T11:41:26.105638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load | Read Data","metadata":{}},{"cell_type":"code","source":"# 2-Load|Read Data\ncsv_path = \"../input/heart-failure-prediction/heart.csv\"\ndf0 = pd.read_csv(csv_path)\ndf = df0.copy() \nfirst_looking(df)\nduplicate_values(df)\ndrop_columns(df,[])\ndrop_null(df, 90)\n# df.head()\n# df.describe().T","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T11:39:21.297893Z","iopub.execute_input":"2021-10-16T11:39:21.298478Z","iopub.status.idle":"2021-10-16T11:39:21.463245Z","shell.execute_reply.started":"2021-10-16T11:39:21.298441Z","shell.execute_reply":"2021-10-16T11:39:21.462201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:39:56.402207Z","iopub.execute_input":"2021-10-16T11:39:56.402974Z","iopub.status.idle":"2021-10-16T11:39:56.425159Z","shell.execute_reply.started":"2021-10-16T11:39:56.40292Z","shell.execute_reply":"2021-10-16T11:39:56.42364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(df);","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:39:58.998616Z","iopub.execute_input":"2021-10-16T11:39:58.998886Z","iopub.status.idle":"2021-10-16T11:39:59.295662Z","shell.execute_reply.started":"2021-10-16T11:39:58.998856Z","shell.execute_reply":"2021-10-16T11:39:59.294651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(df);","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:40:02.10771Z","iopub.execute_input":"2021-10-16T11:40:02.108559Z","iopub.status.idle":"2021-10-16T11:40:02.246053Z","shell.execute_reply.started":"2021-10-16T11:40:02.108507Z","shell.execute_reply":"2021-10-16T11:40:02.244947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:39:53.256247Z","iopub.execute_input":"2021-10-16T11:39:53.256586Z","iopub.status.idle":"2021-10-16T11:39:53.298375Z","shell.execute_reply.started":"2021-10-16T11:39:53.256555Z","shell.execute_reply":"2021-10-16T11:39:53.297127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True)\nplt.xticks(rotation=45);\n# Before deeping into the analysis it would be benefical to examine the correlation among variables using heatmap.","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:41:50.26433Z","iopub.execute_input":"2021-10-16T11:41:50.264713Z","iopub.status.idle":"2021-10-16T11:41:50.974374Z","shell.execute_reply.started":"2021-10-16T11:41:50.264682Z","shell.execute_reply":"2021-10-16T11:41:50.973411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = df.corr()\n\ncount = \"Done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is NO multicollinearity problem\") \n\nprint(\"\\033[1mThe number of strong corelated features:\\033[0m\", count) ","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:39:34.12975Z","iopub.execute_input":"2021-10-16T11:39:34.130065Z","iopub.status.idle":"2021-10-16T11:39:34.160743Z","shell.execute_reply.started":"2021-10-16T11:39:34.130033Z","shell.execute_reply":"2021-10-16T11:39:34.159867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis and Visualization\n","metadata":{}},{"cell_type":"markdown","source":"## Features | Target\n","metadata":{}},{"cell_type":"code","source":"# 3-Target Examination\ntarget = \"heartdisease\"\n\nprint(colored('-'*79, 'red', attrs=['bold']), '\\n',\n      colored(f\"Target(y):\", attrs=['bold']), target, \n      colored(f\"\\nValue Counts:\\n\", attrs=['underline']), df[target].value_counts(),'\\n',\n      colored('-'*79, 'red', attrs=['bold']), sep='')\n\nX_columns = df.drop(target, axis=1).columns\nX_numerical = df.drop(target, axis=1).select_dtypes('number').astype('float64')\nX_categorical = df.drop(target, axis=1).select_dtypes('object')\n\nprint(colored(f\"Feature Columns:\", attrs=['bold']), list(X_columns),'\\n',\n      colored('-'*79, 'red', attrs=['bold']), sep='')\nprint(colored(f\"Numerical Columns:\", attrs=['bold']), list(X_numerical.columns), '\\n',\n      colored('-'*79, 'red', attrs=['bold']), sep='')\nprint(colored(f\"Categorical Columns:\", attrs=['bold']), list(X_categorical.columns), '\\n',\n      colored('-'*79, 'red', attrs=['bold']), sep='')\nprint(colored(f\"Number of Uniques:\", attrs=['bold']), '\\n', X_categorical.nunique(), '\\n',\n      colored('-'*79, 'red', attrs=['bold']), sep='')\n\n# X_categorical.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:42:59.590548Z","iopub.execute_input":"2021-10-16T11:42:59.590869Z","iopub.status.idle":"2021-10-16T11:42:59.6269Z","shell.execute_reply.started":"2021-10-16T11:42:59.59084Z","shell.execute_reply":"2021-10-16T11:42:59.623399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'Percentage of Heart Disease: % \n#      {round(df[target].value_counts(normalize=True)[1]*100,2)}\n#      ({df[target].value_counts()[1]} cases for Heart Disease)\\nPercentage of NOT Heart Disease: % {round(y.value_counts(normalize=True)[0]*100,2)}({df[target].value_counts()[0]} cases for NOT Heart Disease)')\n\n# print( f\"Skewness: {df[target].skew()}\")\n# print( f\"Kurtosis: {df[target].kurtosis()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- So far so good. No zero variance and no extremely high variance.","metadata":{}},{"cell_type":"code","source":"# 10-Train|Test Split, Dummy \n\n# # Before dummy: \n# make_dtype_object = df[['categorical1','categorical2']].astype('object')\n\nX_columns_ = df.drop(target, axis=1).columns\nX_categorical_ = df.drop(target, axis=1).select_dtypes('object')\nX_numerical_ = df.drop(target, axis=1).select_dtypes('number').astype('float64')\n\n###############################################################################\n\nif (df.dtypes==object).any():\n    dummied = pd.get_dummies(X_categorical_, drop_first=True)\n    X = pd.concat([X_numerical_, dummied[dummied.columns]], axis=1)\n    \nelse:\n    X = df.drop(target, axis=1).astype('float64')\ntry:\n    if (df[target].dtypes==object).any():\n        y = pd.get_dummies(df[target], drop_first=True)\n    \nexcept:\n    y = df[target]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=42)\n\n###############################################################################\n\n# # 11-MinMax Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 11-Standart Scaling\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.transform(X_test)\n\n###############################################################################","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:53:05.334873Z","iopub.execute_input":"2021-10-16T11:53:05.335188Z","iopub.status.idle":"2021-10-16T11:53:05.372913Z","shell.execute_reply.started":"2021-10-16T11:53:05.335158Z","shell.execute_reply":"2021-10-16T11:53:05.371895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:53:27.037136Z","iopub.execute_input":"2021-10-16T11:53:27.037446Z","iopub.status.idle":"2021-10-16T11:53:27.056149Z","shell.execute_reply.started":"2021-10-16T11:53:27.037415Z","shell.execute_reply":"2021-10-16T11:53:27.055226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape_control()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:54:31.805744Z","iopub.execute_input":"2021-10-16T11:54:31.806052Z","iopub.status.idle":"2021-10-16T11:54:31.819535Z","shell.execute_reply.started":"2021-10-16T11:54:31.806015Z","shell.execute_reply":"2021-10-16T11:54:31.818259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr().style.background_gradient(cmap='RdPu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Target Variable</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"y = df['HeartDisease']\nprint(f'Percentage of patient had a HeartDisease:  {round(y.value_counts(normalize=True)[1]*100,2)} %  --> ({y.value_counts()[1]} patient)\\nPercentage of patient did not have a HeartDisease: {round(y.value_counts(normalize=True)[0]*100,2)}  %  --> ({y.value_counts()[0]} patient)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Almost 55% of the patients had a heart disease.\n-  508 patient had a heart disease.\n- Almost 45%  of patients didn't have a heart disease.\n- 410 patient didn't have a heart disease.\n","metadata":{}},{"cell_type":"code","source":"df['HeartDisease'].iplot(kind='hist')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is a little imblanace but nothing in the disturbing level.\n- We can use 'accuracy' metric as our evaluation metric.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Numerical Features</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"df[numerical].describe()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_numerical.iplot(kind='hist');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[numerical].iplot(kind='histogram',subplots=True,bins=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df[numerical].drop('FastingBS', axis=1).skew()\nskew_cols= skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Nothing much for the skewness. Quite a normal like distribution for the numerical features.","metadata":{}},{"cell_type":"code","source":"numerical1= df.select_dtypes('number').columns\n\n\nmatrix = np.triu(df[numerical1].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (df[numerical1].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on the  matrix, we can observe weak level correlation between the numerical features and the target variable\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- interestingly cholesterol has negative correlation with the heart disease.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Categorical Features</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"    Table of Contents\n    · The Data\n    · Categorical Distribution Plots\n    ∘ Box Plots\n    ∘ Violin Plots\n    ∘ Boxen Plot\n    · Categorical Estimate Plots\n    ∘ Bar Plot\n    ∘ Point Plot\n    ∘ Count Plot\n    · Categorical Scatter Plots\n    ∘ Strip Plot\n    ∘ Swarm Plot\n    · Combining Plots","metadata":{}},{"cell_type":"markdown","source":"# The Examination of Categorical Features\n","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(df, x=target)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:02:06.532975Z","iopub.execute_input":"2021-10-16T12:02:06.533312Z","iopub.status.idle":"2021-10-16T12:02:07.555659Z","shell.execute_reply.started":"2021-10-16T12:02:06.533281Z","shell.execute_reply":"2021-10-16T12:02:07.554472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[target].iplot(kind='hist')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:02:29.507239Z","iopub.execute_input":"2021-10-16T12:02:29.507538Z","iopub.status.idle":"2021-10-16T12:02:29.629144Z","shell.execute_reply.started":"2021-10-16T12:02:29.507505Z","shell.execute_reply":"2021-10-16T12:02:29.627777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[target].value_counts())\ndf[target].value_counts().plot(kind=\"pie\", autopct='%1.1f%%',figsize=(10,10));","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:03:02.152206Z","iopub.execute_input":"2021-10-16T12:03:02.152489Z","iopub.status.idle":"2021-10-16T12:03:02.307081Z","shell.execute_reply.started":"2021-10-16T12:03:02.152458Z","shell.execute_reply":"2021-10-16T12:03:02.306144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_numerical.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:03:37.34953Z","iopub.execute_input":"2021-10-16T12:03:37.350046Z","iopub.status.idle":"2021-10-16T12:03:37.527267Z","shell.execute_reply.started":"2021-10-16T12:03:37.35Z","shell.execute_reply":"2021-10-16T12:03:37.526348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df[target]==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:04:02.608946Z","iopub.execute_input":"2021-10-16T12:04:02.609532Z","iopub.status.idle":"2021-10-16T12:04:02.659047Z","shell.execute_reply.started":"2021-10-16T12:04:02.609494Z","shell.execute_reply":"2021-10-16T12:04:02.658011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_numerical.iplot(kind='hist')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:04:29.946566Z","iopub.execute_input":"2021-10-16T12:04:29.946977Z","iopub.status.idle":"2021-10-16T12:04:30.195907Z","shell.execute_reply.started":"2021-10-16T12:04:29.946933Z","shell.execute_reply":"2021-10-16T12:04:30.195101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_numerical.iplot(kind='histogram',subplots=True,bins=50)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:04:53.238027Z","iopub.execute_input":"2021-10-16T12:04:53.238535Z","iopub.status.idle":"2021-10-16T12:04:53.530542Z","shell.execute_reply.started":"2021-10-16T12:04:53.238501Z","shell.execute_reply":"2021-10-16T12:04:53.52935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.drop(columns=[target]).columns:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:05:27.242358Z","iopub.execute_input":"2021-10-16T12:05:27.242906Z","iopub.status.idle":"2021-10-16T12:05:28.097487Z","shell.execute_reply.started":"2021-10-16T12:05:27.242856Z","shell.execute_reply":"2021-10-16T12:05:28.096287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 0\nplt.figure(figsize=(20, 20))\nfor feature in df.columns:\n    if feature != target:\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.boxplot(x=target, y=feature, data=df)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:07:41.321839Z","iopub.execute_input":"2021-10-16T12:07:41.322168Z","iopub.status.idle":"2021-10-16T12:07:43.646346Z","shell.execute_reply.started":"2021-10-16T12:07:41.322135Z","shell.execute_reply":"2021-10-16T12:07:43.645128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in X_numerical:\n    if feature != target:\n        index += 1\n        plt.subplot(4,3,index)\n        sns.boxplot(x=target,y=feature,data=df)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:08:37.804603Z","iopub.execute_input":"2021-10-16T12:08:37.804919Z","iopub.status.idle":"2021-10-16T12:08:38.856499Z","shell.execute_reply.started":"2021-10-16T12:08:37.804889Z","shell.execute_reply":"2021-10-16T12:08:38.855627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig = px.scatter_3d(df, \n#                     x='Hardness',\n#                     y='Sulfate',\n#                     z='Chloramines',\n#                     color='Potability')\n# fig.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:03.163791Z","iopub.execute_input":"2021-10-16T12:09:03.164095Z","iopub.status.idle":"2021-10-16T12:09:03.168605Z","shell.execute_reply.started":"2021-10-16T12:09:03.164041Z","shell.execute_reply":"2021-10-16T12:09:03.167412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_categorical.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_columns","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:14.312684Z","iopub.execute_input":"2021-10-16T12:09:14.313043Z","iopub.status.idle":"2021-10-16T12:09:14.320128Z","shell.execute_reply.started":"2021-10-16T12:09:14.313012Z","shell.execute_reply":"2021-10-16T12:09:14.319024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:11.913419Z","iopub.execute_input":"2021-10-16T12:09:11.913727Z","iopub.status.idle":"2021-10-16T12:09:11.921695Z","shell.execute_reply.started":"2021-10-16T12:09:11.913694Z","shell.execute_reply":"2021-10-16T12:09:11.920428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='rainbow')\nplt.title(\"Age by Passenger Class, Titanic\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:21.739758Z","iopub.execute_input":"2021-10-16T12:09:21.740177Z","iopub.status.idle":"2021-10-16T12:09:22.151389Z","shell.execute_reply.started":"2021-10-16T12:09:21.740134Z","shell.execute_reply":"2021-10-16T12:09:22.150314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.violinplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='rainbow', split='True')\nplt.title(\"Violin Plot of Age by Class, Separated by Sex\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:25.747385Z","iopub.execute_input":"2021-10-16T12:09:25.747976Z","iopub.status.idle":"2021-10-16T12:09:26.106Z","shell.execute_reply.started":"2021-10-16T12:09:25.747943Z","shell.execute_reply":"2021-10-16T12:09:26.105083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxenplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='rainbow')\nplt.title(\"Distribution of Age by Passenger Class, Separated by Survival\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:29.348729Z","iopub.execute_input":"2021-10-16T12:09:29.349229Z","iopub.status.idle":"2021-10-16T12:09:29.763198Z","shell.execute_reply.started":"2021-10-16T12:09:29.349176Z","shell.execute_reply":"2021-10-16T12:09:29.762194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.barplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='rainbow')\nplt.title(\"Fare of Passenger by Embarked Town, Divided by Class\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:33.008453Z","iopub.execute_input":"2021-10-16T12:09:33.00875Z","iopub.status.idle":"2021-10-16T12:09:33.500178Z","shell.execute_reply.started":"2021-10-16T12:09:33.0087Z","shell.execute_reply":"2021-10-16T12:09:33.499125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.pointplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='rainbow')\nplt.title(\"Average Fare Price by Embarked Town, Separated by Sex\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:09:37.039634Z","iopub.execute_input":"2021-10-16T12:09:37.040297Z","iopub.status.idle":"2021-10-16T12:09:37.55246Z","shell.execute_reply.started":"2021-10-16T12:09:37.040256Z","shell.execute_reply":"2021-10-16T12:09:37.551442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:11:00.734434Z","iopub.execute_input":"2021-10-16T12:11:00.734959Z","iopub.status.idle":"2021-10-16T12:11:00.74909Z","shell.execute_reply.started":"2021-10-16T12:11:00.734926Z","shell.execute_reply":"2021-10-16T12:11:00.747805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(x=target, data=df, hue='sex', palette='rainbow')\nplt.title(\"Count of Passengers that Embarked in Each City, Separated by Sex\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:12:06.789296Z","iopub.execute_input":"2021-10-16T12:12:06.789585Z","iopub.status.idle":"2021-10-16T12:12:07.047023Z","shell.execute_reply.started":"2021-10-16T12:12:06.789553Z","shell.execute_reply":"2021-10-16T12:12:07.046145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.stripplot(x='heartdisease',y=\"age\",data=df, hue='sex', palette='viridis', jitter=True,  dodge=True )\nplt.title(\"Age by Passenger Class, Separated by Survival\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:12:13.81605Z","iopub.execute_input":"2021-10-16T12:12:13.817092Z","iopub.status.idle":"2021-10-16T12:12:14.425274Z","shell.execute_reply.started":"2021-10-16T12:12:13.817021Z","shell.execute_reply":"2021-10-16T12:12:14.424326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.swarmplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis', dodge=True)\nplt.title(\"Age by Passenger Class, Separated by Survival\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:12:56.364821Z","iopub.execute_input":"2021-10-16T12:12:56.365152Z","iopub.status.idle":"2021-10-16T12:12:57.285464Z","shell.execute_reply.started":"2021-10-16T12:12:56.365119Z","shell.execute_reply":"2021-10-16T12:12:57.284334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis', split='True')\nsns.swarmplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis', dodge='True', color='grey', alpha=.8, s=4)\nplt.title(\"Age by Passenger Class, Separated by Survival\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T12:13:52.013728Z","iopub.execute_input":"2021-10-16T12:13:52.014022Z","iopub.status.idle":"2021-10-16T12:13:53.124487Z","shell.execute_reply.started":"2021-10-16T12:13:52.013992Z","shell.execute_reply":"2021-10-16T12:13:53.123388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis')\nsns.swarmplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis', dodge=True,alpha=.8,color='grey',s=4)\nplt.title(\"Age by Passenger Class, Separated by Survival\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:13:56.862659Z","iopub.execute_input":"2021-10-16T12:13:56.862943Z","iopub.status.idle":"2021-10-16T12:13:57.957013Z","shell.execute_reply.started":"2021-10-16T12:13:56.862911Z","shell.execute_reply":"2021-10-16T12:13:57.956093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.barplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis')\nsns.stripplot(x=target,y=\"age\",data=df, hue='sex', palette='viridis', dodge='True', color='grey', alpha=.8, s=2)\nplt.title(\"Fare of Passenger by Embarked Town, Divided by Class\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:14:12.110291Z","iopub.execute_input":"2021-10-16T12:14:12.110582Z","iopub.status.idle":"2021-10-16T12:14:12.831579Z","shell.execute_reply.started":"2021-10-16T12:14:12.110552Z","shell.execute_reply":"2021-10-16T12:14:12.830703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=target,y='age', col = 'sex', data=df,\n                kind='bar', aspect=.6, palette='Set2')\n(g.set_axis_labels(\"Class\", \"Survival Rate\")\n  .set_titles(\"{col_name}\")\n  .set(ylim=(0,1)))\nplt.tight_layout()\nplt.savefig('seaborn_catplot.png', dpi=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:15:51.828109Z","iopub.execute_input":"2021-10-16T12:15:51.828412Z","iopub.status.idle":"2021-10-16T12:15:52.701814Z","shell.execute_reply.started":"2021-10-16T12:15:51.82838Z","shell.execute_reply":"2021-10-16T12:15:52.700701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Gender and Heart Disease**","metadata":{}},{"cell_type":"code","source":"print (f'A female person has a probability of {round(df[df[\"sex\"]==\"F\"][\"heartdisease\"].mean()*100,2)} % have a HeartDisease')\n\nprint()\n\nprint (f'A male person has a probability of {round(df[df[\"sex\"]==\"M\"][\"heartdisease\"].mean()*100,2)} % have a HeartDisease')\n\nprint()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T12:16:34.111864Z","iopub.execute_input":"2021-10-16T12:16:34.112183Z","iopub.status.idle":"2021-10-16T12:16:34.122622Z","shell.execute_reply.started":"2021-10-16T12:16:34.112152Z","shell.execute_reply":"2021-10-16T12:16:34.121484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"sex\", color=\"heartdisease\",width=400, height=400)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T12:16:50.051737Z","iopub.execute_input":"2021-10-16T12:16:50.052094Z","iopub.status.idle":"2021-10-16T12:16:50.146668Z","shell.execute_reply.started":"2021-10-16T12:16:50.052026Z","shell.execute_reply":"2021-10-16T12:16:50.145283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Bad news guys....\n- Men are almost 2.44 times more likely have a heart disease than women.","metadata":{}},{"cell_type":"markdown","source":"### **Chest Pain Type and Heart Disease**","metadata":{}},{"cell_type":"code","source":"df.groupby('chestpaintype')['heartdisease'].mean().sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T12:17:50.690903Z","iopub.execute_input":"2021-10-16T12:17:50.691411Z","iopub.status.idle":"2021-10-16T12:17:50.707366Z","shell.execute_reply.started":"2021-10-16T12:17:50.691375Z","shell.execute_reply":"2021-10-16T12:17:50.706047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"chestpaintype\", color=\"heartdisease\",width=400, height=400)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T12:18:29.824438Z","iopub.execute_input":"2021-10-16T12:18:29.824724Z","iopub.status.idle":"2021-10-16T12:18:29.919198Z","shell.execute_reply.started":"2021-10-16T12:18:29.824692Z","shell.execute_reply":"2021-10-16T12:18:29.918045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can observe clear differences among the chest pain type.\n- Person with ASY: Asymptomatic chest pain  has almost 6 times more likely have a heart disease than person with ATA Atypical Angina chest pain.\n","metadata":{}},{"cell_type":"markdown","source":"### **RestingECG and Heart Disease**","metadata":{}},{"cell_type":"code","source":"df.groupby('RestingECG')['HeartDisease'].mean().sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:21.902505Z","iopub.execute_input":"2021-10-02T19:00:21.902747Z","iopub.status.idle":"2021-10-02T19:00:21.911788Z","shell.execute_reply.started":"2021-10-02T19:00:21.902714Z","shell.execute_reply":"2021-10-02T19:00:21.910971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"RestingECG\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:21.913334Z","iopub.execute_input":"2021-10-02T19:00:21.913611Z","iopub.status.idle":"2021-10-02T19:00:22.005148Z","shell.execute_reply.started":"2021-10-02T19:00:21.91358Z","shell.execute_reply":"2021-10-02T19:00:22.004325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- RestingECG: resting electrocardiogram results don't differ much.\n- Person with ST: having ST-T wave abnormality is more likely have a heart disease than the others.","metadata":{}},{"cell_type":"markdown","source":"### **ExerciseAngina and Heart Disease**","metadata":{}},{"cell_type":"code","source":"df.groupby('ExerciseAngina')['HeartDisease'].mean().sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:22.00629Z","iopub.execute_input":"2021-10-02T19:00:22.006535Z","iopub.status.idle":"2021-10-02T19:00:22.015559Z","shell.execute_reply.started":"2021-10-02T19:00:22.006506Z","shell.execute_reply":"2021-10-02T19:00:22.014766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"ExerciseAngina\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:22.016975Z","iopub.execute_input":"2021-10-02T19:00:22.017198Z","iopub.status.idle":"2021-10-02T19:00:22.106717Z","shell.execute_reply.started":"2021-10-02T19:00:22.017173Z","shell.execute_reply":"2021-10-02T19:00:22.105947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ExerciseAngina: exercise-induced angina with 'Yes' almost 2.4 times more likley have a heart disaese than exercise-induced angina with 'No'","metadata":{}},{"cell_type":"markdown","source":"### **ST_Slope and Heart Disease**","metadata":{}},{"cell_type":"code","source":"df.groupby('ST_Slope')['HeartDisease'].mean().sort_values(ascending=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:22.108021Z","iopub.execute_input":"2021-10-02T19:00:22.108242Z","iopub.status.idle":"2021-10-02T19:00:22.117972Z","shell.execute_reply.started":"2021-10-02T19:00:22.108217Z","shell.execute_reply":"2021-10-02T19:00:22.117451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"ST_Slope\", color=\"HeartDisease\",width=400, height=400)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:22.119134Z","iopub.execute_input":"2021-10-02T19:00:22.119505Z","iopub.status.idle":"2021-10-02T19:00:22.211605Z","shell.execute_reply.started":"2021-10-02T19:00:22.11946Z","shell.execute_reply":"2021-10-02T19:00:22.210585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ST_Slope: the slope of the peak exercise ST segment has differences.\n-  ST_Slope Up significantly less likely has heart disease than the other two segment.","metadata":{}},{"cell_type":"markdown","source":"### Overall Insights from the Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"- Target variable has close to balanced data.\n- Numerical features have weak correlation with the target variable.\n- Oldpeak (depression related number) has a positive correlation with the heart disease.\n- Maximum heart rate has negative correlation with the heart disease.\n- Interestingly cholesterol has negative correlation with the heart disease.\n- Based on the gender; Men are almost 2.44 times more likely have a heart disease than women.\n- We can observe clear differences among the chest pain type.\n- Person with ASY: Asymptomatic chest pain  has almost 6 times more likely have a heart disease than person with ATA Atypical Angina chest pain.\n- RestingECG: resting electrocardiogram results don't differ much.\n- Person with ST: having ST-T wave abnormality is more likely have a heart disease than the others.\n- ExerciseAngina: exercise-induced angina with 'Yes' almost 2.4 times more likley have a heart disaese than exercise-induced angina with 'No'\n- ST_Slope: the slope of the peak exercise ST segment has differences.\n- ST_Slope Up significantly less likely has heart disease than the other two segment.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>MODEL SELECTION</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- We'll use dummy classifier model as a base model\n-  And then we will use Logistic & Linear Discriminant & KNeighbors and Support Vector Machine models with and without scaler.\n- And then we will use ensemble models, Adaboost, Randomforest, Gradient Boosting and Extra Trees\n- We will see famous trio: XGBoost,LightGBM & Catboost\n- Finally we will look in detail to hyperparameter tuning for Catboost\n- Let's start.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Baseline Model</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify= random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nmodel = DummyClassifier(strategy='constant', constant=1)\npipe = make_pipeline(ct, model)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['DummyClassifier']\ndummy_result_df = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\ndummy_result_df","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:22.21335Z","iopub.execute_input":"2021-10-02T19:00:22.21369Z","iopub.status.idle":"2021-10-02T19:00:22.249091Z","shell.execute_reply.started":"2021-10-02T19:00:22.213652Z","shell.execute_reply":"2021-10-02T19:00:22.248264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Logistic & Linear Discriminant & SVC & KNN</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\n\nlr = LogisticRegression(solver='liblinear')\nlda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,lda,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic','LinearDiscriminant','SVM','KNeighbors']\nresult_df1 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df1","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:22.250644Z","iopub.execute_input":"2021-10-02T19:00:22.25089Z","iopub.status.idle":"2021-10-02T19:00:22.425568Z","shell.execute_reply.started":"2021-10-02T19:00:22.25086Z","shell.execute_reply":"2021-10-02T19:00:22.424558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<font color=\"lightseagreen\" size=+1.5><b> Logistic & Linear Discriminant & SVC & KNN with Scaler</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\ns= StandardScaler()\nct1= make_column_transformer((ohe,categorical),(s,numerical))  \n\n\nlr = LogisticRegression(solver='liblinear')\nlda= LinearDiscriminantAnalysis()\nsvm = SVC(gamma='scale')\nknn = KNeighborsClassifier()\n\nmodels = [lr,lda,svm,knn]\n\nfor model in models: \n    pipe = make_pipeline(ct1, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Logistic_scl','LinearDiscriminant_scl','SVM_scl','KNeighbors_scl']\nresult_df2 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df2","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:22.430959Z","iopub.execute_input":"2021-10-02T19:00:22.431231Z","iopub.status.idle":"2021-10-02T19:00:22.617613Z","shell.execute_reply.started":"2021-10-02T19:00:22.431196Z","shell.execute_reply":"2021-10-02T19:00:22.616636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected, with scaler, both KNN and SVM did a better job with the scaler than their previous performances.","metadata":{}},{"cell_type":"markdown","source":"- Let's see how ensemble models do with the problem at hand.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Ensemble Models (AdaBoost & Gradient Boosting & Random Forest & Extra Trees)</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nada = AdaBoostClassifier(random_state=0)\ngb = GradientBoostingClassifier(random_state=0)\nrf = RandomForestClassifier(random_state=0)\net=  ExtraTreesClassifier(random_state=0)\n\n\n\nmodels = [ada,gb,rf,et]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n    print (f'model : {model} and  accuracy score is : {round(accuracy_score(y_test, y_pred),4)}')\n\nmodel_names = ['Ada','Gradient','Random','ExtraTree']\nresult_df3 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df3","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:22.619189Z","iopub.execute_input":"2021-10-02T19:00:22.619521Z","iopub.status.idle":"2021-10-02T19:00:23.381132Z","shell.execute_reply.started":"2021-10-02T19:00:22.619476Z","shell.execute_reply":"2021-10-02T19:00:23.380409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Accuracy scores are very close to each other.\n- Both Random Forest and Extra tree got similar accuracy scores.\n- Both model can be improved by hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"- OK. Let's see the very famous trio:\n  - XGBoost\n  - Light GBM\n  - Catboost","metadata":{}},{"cell_type":"markdown","source":"<a id=\"11\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Famous Trio (XGBoost & LightGBM & Catboost)</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- I'll use Catboost alone by using its capability to handle categorical variables without doing any preprocessing.\n- Let's first look at the XGBoost and LightGBM","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nohe= OneHotEncoder()\nct= make_column_transformer((ohe,categorical),remainder='passthrough')  \n\nxgbc = XGBClassifier(random_state=0)\nlgbmc=LGBMClassifier(random_state=0)\n\n\nmodels = [xgbc,lgbmc]\n\nfor model in models: \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost','LightGBM']\nresult_df4 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df4","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:23.382469Z","iopub.execute_input":"2021-10-02T19:00:23.38268Z","iopub.status.idle":"2021-10-02T19:00:23.597692Z","shell.execute_reply.started":"2021-10-02T19:00:23.382656Z","shell.execute_reply":"2021-10-02T19:00:23.596937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- With their deafult values, Catboost did better job than the other two models.","metadata":{}},{"cell_type":"markdown","source":"- Now let's see Catboost","metadata":{}},{"cell_type":"markdown","source":"![](https://avatars.mds.yandex.net/get-bunker/56833/dba868860690e7fe8b68223bb3b749ed8a36fbce/orig)","metadata":{}},{"cell_type":"markdown","source":"image credit: https://avatars.mds.yandex.net","metadata":{}},{"cell_type":"markdown","source":"<a id=\"12\"></a>\n<font color=\"lightseagreen\" size=+1.5><b> CATBOOST</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"**Purpose**: \n   \n   Training and applying models for the classification problems. Provides compatibility with the scikit-learn tools.\n\n**The default optimized objective depends on various conditions**:\n\n**Logloss** — The target has only two different values or the target_border parameter is not None.\n\n**MultiClass** — The target has more than two different values and the border_count parameter is None.\n\nReference: https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier\n","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0)\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_default']\nresult_df5 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df5\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:00:23.601163Z","iopub.execute_input":"2021-10-02T19:00:23.603143Z","iopub.status.idle":"2021-10-02T19:00:27.204901Z","shell.execute_reply.started":"2021-10-02T19:00:23.603099Z","shell.execute_reply":"2021-10-02T19:00:27.204325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Let's make some adjustment on the Catboost model to see its' peak performance on the problem.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"13\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Catboost HyperParameter Tuning with Optuna</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    X= df.drop('HeartDisease', axis=1)\n    y= df['HeartDisease']\n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    cat_cls = CatBoostClassifier(**param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:12:27.434934Z","iopub.execute_input":"2021-10-02T19:12:27.43538Z","iopub.status.idle":"2021-10-02T19:13:16.378317Z","shell.execute_reply.started":"2021-10-02T19:12:27.435349Z","shell.execute_reply":"2021-10-02T19:13:16.377306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Parameters**:\n\n- **Objective**:  Supported metrics for overfitting detection and best model selection \n\n- **colsample_bylevel**: this parameter speeds up the training and usually does not affect the quality.\n\n- **depht** : Depth of the tree.\n\n\n- **boosting_type** : By default, the boosting type is set to for small datasets. This prevents overfitting but it is expensive in terms of computation. Try to set the value of this parameter to  to speed up the training.\n\n- **bootstrap_type** : By default, the method for sampling the weights of objects is set to . The training is performed faster if the method is set and the value for the sample rate for bagging is smaller than 1.\n\n\nReference: https://catboost.ai/\n\n\n","metadata":{}},{"cell_type":"markdown","source":"- Ok let's use our best model with new parameters.","metadata":{}},{"cell_type":"code","source":"accuracy =[]\nmodel_names =[]\n\n\nX= df.drop('HeartDisease', axis=1)\ny= df['HeartDisease']\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = CatBoostClassifier(verbose=False,random_state=0,\n                          objective= 'CrossEntropy',\n    colsample_bylevel= 0.04292240490294766,\n    depth= 10,\n    boosting_type= 'Plain',\n    bootstrap_type= 'MVS')\n\nmodel.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nprint(classification_report(y_test, y_pred))\n\nmodel_names = ['Catboost_tuned']\nresult_df6 = pd.DataFrame({'Accuracy':accuracy}, index=model_names)\nresult_df6\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:05:53.538048Z","iopub.execute_input":"2021-10-02T19:05:53.539105Z","iopub.status.idle":"2021-10-02T19:05:54.26392Z","shell.execute_reply.started":"2021-10-02T19:05:53.539049Z","shell.execute_reply":"2021-10-02T19:05:54.262936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have lift from 0.8804 to .9094","metadata":{}},{"cell_type":"markdown","source":"<a id=\"14\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Feature Importance</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"feature_importance = np.array(model.get_feature_importance())\nfeatures = np.array(X_train.columns)\nfi={'features':features,'feature_importance':feature_importance}\ndf_fi = pd.DataFrame(fi)\ndf_fi.sort_values(by=['feature_importance'], ascending=True,inplace=True)\nfig = px.bar(df_fi, x='feature_importance', y='features',title=\"CatBoost Feature Importance\",height=500)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:45.443974Z","iopub.execute_input":"2021-10-02T19:00:45.444379Z","iopub.status.idle":"2021-10-02T19:00:45.526771Z","shell.execute_reply.started":"2021-10-02T19:00:45.444348Z","shell.execute_reply":"2021-10-02T19:00:45.525892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a>\n<font color=\"lightseagreen\" size=+1.5><b>Model Comparison</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"result_final = pd.concat([dummy_result_df,result_df1,result_df2,result_df3,result_df4,result_df5,result_df6],axis=0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:45.528154Z","iopub.execute_input":"2021-10-02T19:00:45.528415Z","iopub.status.idle":"2021-10-02T19:00:45.534422Z","shell.execute_reply.started":"2021-10-02T19:00:45.528368Z","shell.execute_reply":"2021-10-02T19:00:45.533622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_final.sort_values(by=['Accuracy'], ascending=True,inplace=True)\nfig = px.bar(result_final, x='Accuracy', y=result_final.index,title='Model Comparison',height=600,labels={'index':'MODELS'})\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-02T19:00:45.53617Z","iopub.execute_input":"2021-10-02T19:00:45.53696Z","iopub.status.idle":"2021-10-02T19:00:45.616323Z","shell.execute_reply.started":"2021-10-02T19:00:45.536926Z","shell.execute_reply":"2021-10-02T19:00:45.61569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"16\"></a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n","metadata":{}},{"cell_type":"markdown","source":"- We have developed model to classifiy heart disease cases.\n\n- First, we  made the detailed exploratory analysis.\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem on hand.\n- We looked in detail Catboost\n- We made hyperparameter tuning of the Catboost with Optuna to see the improvement\n- We looked at the feature importance.\n\n\n\n- After this point it is up to you to develop and improve the models.  **Enjoy** 🤘","metadata":{"execution":{"iopub.status.busy":"2021-10-02T18:05:22.40737Z","iopub.execute_input":"2021-10-02T18:05:22.407997Z","iopub.status.idle":"2021-10-02T18:05:22.416557Z","shell.execute_reply.started":"2021-10-02T18:05:22.407953Z","shell.execute_reply":"2021-10-02T18:05:22.415522Z"}}},{"cell_type":"markdown","source":"#### By the way, when you like the topic, you can show it by supporting 👍\n\n####  **Feel free to leave a comment**. \n\n#### All the best 🤘","metadata":{}},{"cell_type":"markdown","source":"![](https://media.giphy.com/media/3o7TKUM3IgJBX2as9O/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"gif credit: https://media.giphy.com/","metadata":{}},{"cell_type":"markdown","source":"<a id=\"17\"></a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading</b></font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https://www.kaggle.com/general/255972)","metadata":{}}]}